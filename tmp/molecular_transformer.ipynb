{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bcfde7-c774-4ca3-8b1d-01b1df546128",
   "metadata": {},
   "source": [
    "# Molecular Transformer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edb270a-d110-429f-86db-cbfd43aa8e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanfunk/miniforge3/envs/erxn/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db15c8f-f742-4ad2-aa93-a96012b37ec0",
   "metadata": {},
   "source": [
    "## Smile tokenizer\n",
    "\n",
    "Canonical smiles were tokenized using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f489ad-3266-4320-a5cc-32f65d98f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a0575-4daf-4905-9534-616a833f0971",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "the dataset of canonical smiles, split in source and target are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef800fa4-a306-47c5-8260-479c97650e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: C ( C ) N ( C C ) C C . C ( C ) S ( Cl ) ( = O ) = O . C C O C C . O C C Br\n",
      "\n",
      "tgt: C C S ( = O ) ( = O ) O C C Br\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_training_data_path = '../data/mol_transformer/data/STEREO_mixed_augm/src-train.txt'\n",
    "src_test_data_path = '../data/mol_transformer/data/STEREO_mixed_augm/src-test.txt'\n",
    "src_valid_data_path = '../data/mol_transformer/data/STEREO_mixed_augm/src-val.txt'\n",
    "tgt_training_data_path = '../data/mol_transformer/data/STEREO_mixed_augm/tgt-train.txt'\n",
    "tgt_test_data_path = '../data/mol_transformer/data/STEREO_mixed_augm/tgt-test.txt'\n",
    "tgt_valid_data_path = '../data/mol_transformer/data/STEREO_mixed_augm/tgt-val.txt'\n",
    "\n",
    "src = ''\n",
    "with open(src_training_data_path, 'r') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        src = line\n",
    "        break\n",
    "\n",
    "tgt = ''\n",
    "with open(tgt_training_data_path, 'r') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        tgt = line\n",
    "        break\n",
    "        \n",
    "print('src:', src)\n",
    "print('tgt:', tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0360f83-2f43-4589-be80-8cacc6805ada",
   "metadata": {},
   "source": [
    "## Building a vocabulary\n",
    "\n",
    "A vocabulary needs to be created for the tokenized smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d694dc8b-7131-43e3-a359-b35a6212c50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3811102lines [00:09, 382913.69lines/s]\n"
     ]
    }
   ],
   "source": [
    "files = [src_training_data_path,\n",
    "         src_test_data_path,\n",
    "         src_valid_data_path,\n",
    "         tgt_training_data_path,\n",
    "         tgt_test_data_path,\n",
    "         tgt_valid_data_path\n",
    "        ]\n",
    "\n",
    "def yield_tokens():\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            for example in f:\n",
    "                tokens = example.replace('\\n','').split(' ')\n",
    "                yield tokens\n",
    "\n",
    "token_generator = yield_tokens()\n",
    "\n",
    "vocab = build_vocab_from_iterator(token_generator)\n",
    "#vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386c955-518c-4e8a-b80b-46656362311d",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0a7f6e5-f5e3-42a2-9740-7f9cb012bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RxnDataset(Dataset):\n",
    "    def __init__(self, src_path, tgt_path, vocab):\n",
    "        self.src_data_path = src_path\n",
    "        self.tgt_data_path = tgt_path\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        with open(self.src_data_path, 'r') as f:\n",
    "            return len(f.readlines())\n",
    "\n",
    "    def __getitem__(self, index, sos_token=0):\n",
    "        src_path = self.src_data_path\n",
    "        tgt_path = self.tgt_data_path\n",
    "        vocab = self.vocab\n",
    "        \n",
    "        src = linecache.getline(src_path, index + 1) # linecache indexing starts at 1 for some reason\n",
    "        tgt = linecache.getline(tgt_path, index + 1)\n",
    "        \n",
    "        src = torch.tensor(\n",
    "            [vocab[token] for token in src.replace('\\n','').split(' ')]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        tgt = torch.tensor(\n",
    "            [vocab[token] for token in tgt.replace('\\n','').split(' ')]\n",
    "        )\n",
    "        sos_token = torch.Tensor([sos_token])\n",
    "        tgt =  torch.concat((sos_token,tgt), dim=0).to(int)\n",
    "        return (src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9ce60-f4ee-41c6-8bbb-070a2d0cc540",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loader and collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f19c1ec7-32d6-4222-9117-7eb9cfe9fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RxnDataset(src_training_data_path,\n",
    "                           tgt_training_data_path,\n",
    "                           vocab\n",
    "                          )\n",
    "\n",
    "test_dataset = RxnDataset(src_test_data_path,\n",
    "                           tgt_test_data_path,\n",
    "                           vocab\n",
    "                          )\n",
    "\n",
    "valid_dataset = RxnDataset(src_valid_data_path,\n",
    "                           tgt_valid_data_path,\n",
    "                           vocab\n",
    "                          )\n",
    "\n",
    "train_data_iter = iter(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2fa7ba9d-2bde-4f4b-b9ac-26c03923c7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3,  4,  3,  5, 11,  4,  3,  3,  5,  3,  3, 10,  3,  4,  3,  5, 17,  4,\n",
       "         15,  5,  4,  8,  6,  5,  8,  6, 10,  3,  3,  6,  3,  3, 10,  6,  3,  3,\n",
       "         21]),\n",
       " tensor([ 0,  3,  3, 17,  4,  8,  6,  5,  4,  8,  6,  5,  6,  3,  3, 21]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386afa7-500b-4d11-a2c0-809d8bda1b67",
   "metadata": {},
   "source": [
    "## Data loader and collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81c9c930-8acd-4f48-9a72-989602adf1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "BATCH_SIZE =32\n",
    "\n",
    "def pad_collate(batch, padding_value: int = 1):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=padding_value)\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=pad_collate,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=pad_collate,\n",
    "    shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=pad_collate,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51645510-d2bd-4188-9772-00e5fe5840fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56412 1571 1567\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader), len(test_loader), len(valid_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42955d7b-ac52-43fc-ada0-f0fc8a054f74",
   "metadata": {},
   "source": [
    "## One hot encoder\n",
    "The one hot encoder is later needed to train the model. The model makes predictions for\n",
    "the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe198d7a-e4b4-45f5-a53d-945d91a5abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(v: Tensor, vocab_size: int) -> Tensor:\n",
    "    '''\n",
    "    Takes tokenized sentences and one hot encodes\n",
    "    them. Tokens have to be integer values.\n",
    "    Args:\n",
    "    -----\n",
    "    v : Tensor\n",
    "        shape (batch_size, seq_length)\n",
    "    Out:\n",
    "    ----\n",
    "    out : Tensor\n",
    "        shape (batch_size, seq_length, vocab_size)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    out = torch.zeros((v.size(0), v.size(1), vocab_size))\n",
    "    for batch in range(v.size(0)):\n",
    "        for i, token in enumerate(v[0,:]):\n",
    "            out[batch,i,token] = 1\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fd566-5f98-43a8-b74a-0bbe8f643500",
   "metadata": {},
   "source": [
    "# Transformer implementation\n",
    "\n",
    "you can ignore that for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4647d7ef-22c2-46c7-add0-7d099ff304f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 208]) torch.Size([32, 78])\n"
     ]
    }
   ],
   "source": [
    "for i, (src, tgt, _, _) in enumerate(train_loader):\n",
    "    # attach to device\n",
    "    break\n",
    "print(src.shape, tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b57d638-7a3a-41e4-8ddd-af44e838f5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 32, 512]) torch.Size([78, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    '''\n",
    "    embeds sentence\n",
    "    Args:\n",
    "    -----\n",
    "    vocab_size : int\n",
    "        size of vocabulary\n",
    "        \n",
    "    embed_dim : int\n",
    "        embedding dimension\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 512):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self, x) -> Tensor:\n",
    "        '''\n",
    "        forward pass\n",
    "        Args:\n",
    "        -----\n",
    "        x : Tensor\n",
    "            shape [batch_size, seq_length]\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : Tensor\n",
    "            shape [seq_length, batch_size, embed_dim]\n",
    "        '''\n",
    "        out = self.embed(x) # (batch_size, seq_length, embed_dim)\n",
    "        out = out.permute(1,0,2) # (seq_length, batch_size, embed_dim)\n",
    "        return out\n",
    "    \n",
    "embedding = Embedding(len(vocab))\n",
    "src_embed = embedding(src)\n",
    "tgt_embed = embedding(tgt)\n",
    "print(src_embed.shape, tgt_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d23723b-69ce-4742-8921-8ee1ea0e863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 32, 512]) torch.Size([78, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    positional encoding\n",
    "    Args:\n",
    "    -----\n",
    "        embed_dim: int\n",
    "            embedding dimension\n",
    "        dropout : float\n",
    "            dropout probability\n",
    "        max_len : int\n",
    "            maximum sequence length\n",
    "    '''\n",
    "    def __init__(self, embed_dim: int = 512, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, 1, embed_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        x: Tensor \n",
    "            shape [seq_len, batch_size, embedding_dim]\n",
    "        Returns:\n",
    "        --------\n",
    "        out : Tensor\n",
    "            shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        out = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(out)\n",
    "    \n",
    "pos_encoding = PositionalEncoding()\n",
    "src_pos_embed = pos_encoding(src_embed)\n",
    "tgt_pos_embed = pos_encoding(tgt_embed)\n",
    "print(src_pos_embed.shape, tgt_pos_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6955420-034e-4043-a012-c3cc1cff5f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 32, 512]) torch.Size([32, 8, 208, 64]) torch.Size([32, 8, 208, 64]) torch.Size([32, 8, 208, 64])\n",
      "torch.Size([78, 32, 512]) torch.Size([32, 8, 78, 64]) torch.Size([32, 8, 78, 64]) torch.Size([32, 8, 78, 64])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    '''SelfAttention mechanism.\n",
    "    Args:\n",
    "    -----\n",
    "    dim : int\n",
    "        The out dimension of the query, key and value.\n",
    "    n_heads : int\n",
    "        Number of self-attention heads.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "    '''\n",
    "    def __init__(self, dim: int = 512, n_heads: int = 8, qkv_bias: bool = True, \n",
    "                 attn_p: float = 0.1, proj_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "    \n",
    "    def forward(self, x, mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "        Args:\n",
    "        -----\n",
    "        x : Tensor\n",
    "            shape [seq_len, batch_size, embedding_dim].\n",
    "        Returns:\n",
    "        --------\n",
    "        x : Tensor\n",
    "            x shape [seq_len, batch_size, embedding_dim].\n",
    "        q, k, v : Tensor\n",
    "            q, k, v shape [batch_size, n_heads, tgt_seq_length, head_dim]\n",
    "        \"\"\"\n",
    "        batch_size, n_tokens, dim = x.shape\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x)  # (seq_length, batch_size, 3 * dim)\n",
    "\n",
    "        qkv = qkv.reshape(\n",
    "            batch_size, n_tokens, 3, self.n_heads, self.head_dim\n",
    "        )  # (batch_size, seq_length + 1, 3, n_heads, head_dim)\n",
    "\n",
    "        qkv = qkv.permute(\n",
    "            2, 1, 3, 0, 4\n",
    "        )  # (3, batch_size, n_heads, seq_length + 1, head_dim)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # (batch_size, n_heads, seq_length, head_dim)\n",
    "\n",
    "        dp = torch.einsum('kabc,qdef->qabe',k , q) * self.scale  # k_t @ q (batch_size, n_heads, seq_length, seq_length)\n",
    "        \n",
    "        if mask is not None:\n",
    "            torch.einsum('xy,abcd->abxy',mask, dp)\n",
    "        \n",
    "        scores = dp.softmax(dim=-1)  # (batch_size, n_heads, seq_length, seq_length)\n",
    "        scores = self.attn_drop(scores)\n",
    "\n",
    "\n",
    "        weighted_avg = torch.einsum('qabc,kdef->bqaf',scores, v) # (batch_size, seq_length, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)  # (seq_length, batch_size, dim)\n",
    "        \n",
    "        x = self.proj(weighted_avg)  # (seq_length, batch_size, dim)\n",
    "        x = self.proj_drop(x)  # (seq_length, batch_size, dim)\n",
    "\n",
    "        return x, q, k, v\n",
    "        \n",
    "self_attention = SelfAttention()\n",
    "src_attn, src_q, src_k, src_v = self_attention(src_pos_embed)\n",
    "tgt_attn, tgt_q, tgt_k, tgt_v = self_attention(tgt_pos_embed)\n",
    "print(src_attn.shape, src_q.shape, src_k.shape, src_v.shape)\n",
    "print(tgt_attn.shape, tgt_q.shape, tgt_k.shape, tgt_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7f76fc62-3496-4212-b074-64716738b70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 32, 512]) torch.Size([32, 8, 208, 64]) torch.Size([32, 8, 208, 64]) torch.Size([32, 8, 208, 64])\n",
      "torch.Size([78, 32, 512]) torch.Size([32, 8, 78, 64]) torch.Size([32, 8, 208, 64]) torch.Size([32, 8, 208, 64])\n"
     ]
    }
   ],
   "source": [
    "class EncoderDecoderAttention(nn.Module):\n",
    "    '''SelfAttention mechanism.\n",
    "    Args\n",
    "    ----\n",
    "    dim : int\n",
    "        The out dimension of the query, key and value.\n",
    "    n_heads : int\n",
    "        Number of self-attention heads.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "    '''\n",
    "    def __init__(self, dim: int = 512, n_heads: int = 8, qkv_bias: bool = True, \n",
    "                 attn_p: float = 0.1, proj_p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_matrix = nn.Linear(dim,dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "    \n",
    "    def forward(self, x, k: Tensor=None, v: Tensor=None, mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "        Args\n",
    "        ----\n",
    "        x : Tensor\n",
    "            shape [seq_len, batch_size, embedding_dim].\n",
    "        Returns\n",
    "        -------\n",
    "        x : Tensor\n",
    "            x shape [seq_len, batch_size, embedding_dim].\n",
    "        q, k, v : Tensor\n",
    "            q, k, v shape [batch_size, n_heads, tgt_seq_length, head_dim]\n",
    "        \"\"\"\n",
    "        batch_size, n_tokens, dim = x.shape\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        q = self.q_matrix(x)  # (tgt_seq_length, batch_size, dim)\n",
    "    \n",
    "        q = q.reshape(\n",
    "            batch_size, n_tokens, self.n_heads, self.head_dim\n",
    "        )  # (tgt_seq_length, batch_size, n_heads, head_dim)\n",
    "        \n",
    "        q = q.permute(\n",
    "            1, 2, 0, 3\n",
    "        )  # (batch_size, n_heads, tgt_seq_length, head_dim)\n",
    "    \n",
    "        dp = torch.einsum('abkd,abqd->abqk',k , q) * self.scale # k_t @ q (batch_size, n_heads, tgt_seq_len, src_seq_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            torch.einsum('xy,abcd->abxy',mask, dp)\n",
    "        \n",
    "        scores = dp.softmax(dim=-1)  # (batch_size, n_heads, seq_length + 1, seq_length + 1)\n",
    "        scores = self.attn_drop(scores)\n",
    "        \n",
    "        weighted_avg = torch.einsum('abts,abse->tabe',scores, v) # (seq_length, batch_size, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)  # (seq_length, batch_size, dim)\n",
    "        \n",
    "        x = self.proj(weighted_avg)  # (seq_length, batch_size, dim)\n",
    "        x = self.proj_drop(x)  # (seq_length, batch_size, dim)\n",
    "\n",
    "        return x, q, k, v\n",
    "\n",
    "k = torch.rand(src_k.shape)\n",
    "v = torch.rand(src_v.shape)\n",
    "encoder_decoder_attention = EncoderDecoderAttention()\n",
    "tgt_attn, tgt_q, tgt_k, tgt_v = encoder_decoder_attention(tgt_pos_embed, k, v)\n",
    "print(src_attn.shape, src_q.shape, src_k.shape, src_v.shape)\n",
    "print(tgt_attn.shape, tgt_q.shape, tgt_k.shape, tgt_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "818fa389-4cbc-49a6-9566-56d29b10143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "    Args\n",
    "    ----\n",
    "    in_features : int\n",
    "        Number of input features.\n",
    "    hidden_features : int\n",
    "        Number of nodes in the hidden layer.\n",
    "    out_features : int\n",
    "        Number of output features.\n",
    "    p : float\n",
    "        Dropout probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int = 512, hidden_features: int = 4*512, \n",
    "                 out_features: int = 512, p: float = 0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "        Args\n",
    "        ----\n",
    "        x : torch.Tensor\n",
    "            Shape `(batch_size, n_patches + 1, in_features)`.\n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batch_size, n_patches +1, out_features)`\n",
    "        \"\"\"\n",
    "        x = self.fc1(\n",
    "            x\n",
    "        )  # (batch_size, n_patches + 1, hidden_features)\n",
    "        x = self.act(x)  # (batch_size, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x)  # (batch_size, n_patches + 1, hidden_features)\n",
    "        x = self.fc2(x)  # (batch_size, n_patches + 1, out_features)\n",
    "        x = self.drop(x)  # (batch_size, n_patches + 1, out_features)\n",
    "\n",
    "        return x\n",
    "\n",
    "mlp = MLP()\n",
    "z = mlp(src_attn)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8da047c-1f9b-4a77-b88e-214da41425b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 32, 512])\n",
      "torch.Size([32, 8, 208, 64]) torch.Size([32, 8, 208, 64])\n"
     ]
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embeddinig dimension.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int = 512, n_heads: int = 8, mlp_ratio: float = 4.0, \n",
    "                 qkv_bias: bool = True, p: float = 0., attn_p: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = SelfAttention(\n",
    "            dim,\n",
    "            n_heads=n_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_p=attn_p,\n",
    "            proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=hidden_features,\n",
    "            out_features=dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batch_size, n_patches + 1, dim)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(batch_size, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        attn, q, k, v = self.attn(x, mask)\n",
    "        attn_add_norm = self.norm1(attn + x)\n",
    "        z = self.mlp(attn_add_norm)\n",
    "        out = self.norm2(z+attn_add_norm)\n",
    "        \n",
    "        return out, k, v\n",
    "\n",
    "encoder_block = EncoderBlock()\n",
    "encoded_src, k, v = encoder_block(src_pos_embed)\n",
    "print(encoded_src.shape)\n",
    "print(k.shape, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ffc2bebf-313c-4413-8ced-22b180db83b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embeddinig dimension.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int = 512, n_heads: int = 8, mlp_ratio: float = 4.0, \n",
    "                 qkv_bias: bool = True, p: float = 0., attn_p: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.self_attn = SelfAttention(\n",
    "            dim,\n",
    "            n_heads=n_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_p=attn_p,\n",
    "            proj_p=p\n",
    "        )\n",
    "        self.encoder_decoder_attn = EncoderDecoderAttention(\n",
    "            dim,\n",
    "            n_heads=n_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_p=attn_p,\n",
    "            proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=hidden_features,\n",
    "            out_features=dim,\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x: Tensor, k: Tensor, v: Tensor , mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batch_size, n_patches + 1, dim)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(batch_size, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        attn, _, _, _ = self.self_attn(x, mask)\n",
    "        \n",
    "        attn_add_norm = self.norm1(attn + x)\n",
    "        attn, _, _, _ = self.encoder_decoder_attn(attn_add_norm, k, v, mask)\n",
    "        attn_add_norm = self.norm2(attn + x)\n",
    "        z = self.mlp(attn_add_norm)\n",
    "        out = self.norm3(z+attn_add_norm)\n",
    "        \n",
    "        return out\n",
    "\n",
    "decoder_block = DecoderBlock()\n",
    "decoded_tgt = decoder_block(tgt_pos_embed, k, v)\n",
    "print(decoded_tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf001610-23dc-4dfc-8df8-bcfa99bb727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 78])\n",
      "torch.Size([32, 78, 405])\n",
      "torch.Size([32, 78, 405])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"The enzyme transformer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embed_dim : int\n",
    "        Dimensionality of the token/patch embeddings.\n",
    "    encoder_depth : int\n",
    "        Number of blocks.\n",
    "    decoder_depth : int\n",
    "        Number of blocks.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension of the `MLP` module.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embed_dim=512,\n",
    "            encoder_depth=8,\n",
    "            decoder_depth=8,\n",
    "            n_heads=8,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            p=0.,\n",
    "            attn_p=0.,\n",
    "            src_masking=False,\n",
    "            tgt_masking=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size=vocab_size)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,\n",
    "                )\n",
    "                for _ in range(encoder_depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,\n",
    "                )\n",
    "                for _ in range(encoder_depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.src_masking = src_masking\n",
    "        self.tgt_masking = tgt_masking\n",
    "        \n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "        self.softmax = F.softmax \n",
    "    \n",
    "    def generate_mask(self, sz: int) -> Tensor:\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"Run the forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(batch_size, in_chans, num_atoms, num_encoding_dimensions)`.\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Logits over all the classes - `(batch_size, n_classes)`.\n",
    "        \"\"\"\n",
    "        if self.src_masking:\n",
    "            src_mask = self.generate_mask(src.size(1))\n",
    "        else:\n",
    "            src_mask = None\n",
    "            \n",
    "        if self.tgt_masking:\n",
    "            tgt_mask = self.generate_mask(tgt.size(1))\n",
    "        else:\n",
    "            tgt_mask = None\n",
    "            \n",
    "        src_embed = self.embedding(src)\n",
    "        src = self.pos_encoding(src_embed)\n",
    "        src = self.pos_drop(src)\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "            src, k, v = block(src, mask=src_mask)\n",
    "        \n",
    "        tgt_embed = self.embedding(tgt)\n",
    "        tgt = self.pos_encoding(tgt_embed)\n",
    "        tgt = self.pos_drop(tgt)\n",
    "        \n",
    "        for block in self.decoder_blocks:\n",
    "            tgt = block(tgt, k, v, mask=tgt_mask)\n",
    "\n",
    "        \n",
    "        out = self.head(tgt)\n",
    "        out = self.softmax(out, dim=-1)\n",
    "        out = out.permute(1,0,2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = Transformer(vocab_size=len(vocab))\n",
    "out = model(src, tgt)\n",
    "tgt_ohe = one_hot_encoder(tgt, vocab_size=len(vocab))\n",
    "print(tgt.shape)\n",
    "print(tgt_ohe.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbadf283-021d-4a9b-8d71-bcafc7f56825",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61db1946-2f6c-432f-ab9a-0f71acc82959",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size,\n",
    "    embed_dim=512,\n",
    "    encoder_depth=8,\n",
    "    decoder_depth=8,\n",
    "    n_heads=8,\n",
    "    mlp_ratio=4.,\n",
    "    qkv_bias=True,\n",
    "    p=0.,\n",
    "    attn_p=0.,\n",
    "    src_masking=False,\n",
    "    tgt_masking=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e433f44-f4de-4e67-ba44-564f9e81fa9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m out \u001b[38;5;241m=\u001b[39m model(src, tgt)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, one_hot_encoder(tgt, vocab_size))\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m~/miniforge3/envs/erxn/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/erxn/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                         betas=(0.9,0.998),\n",
    "                                         lr=1e-3,\n",
    "                                         weight_decay=0.01\n",
    "                                )\n",
    "num_epochs = 10\n",
    "train_loss, test_loss = [], []\n",
    "summary = []\n",
    "for epoch in range(num_epochs):\n",
    "    batch_loss = 0\n",
    "    model.train()\n",
    "    for i, (src, tgt, _, _) in enumerate(train_loader):\n",
    "        # attach to device\n",
    "        src = src.to(device)\n",
    "        trg = tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        out = model(src, tgt)\n",
    "        \n",
    "        loss = criterion(out, one_hot_encoder(tgt, vocab_size))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss += loss.data\n",
    "\n",
    "    train_loss.append(batch_loss / len(train_loader))\n",
    "\n",
    "    batch_loss = 0\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    for i, (src, tgt, _, _) in enumerate(test_loader):\n",
    "        # attach to device\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        pred = model(src, tgt)\n",
    "        loss = criterion(pred, one_hot_encoder(tgt, vocab_size))\n",
    "        batch_loss += loss.data\n",
    "\n",
    "        acc += get_acc(pred, y_test)\n",
    "        \n",
    "    test_loss.append(batch_loss / len(test_loader))\n",
    "    acc = acc / len(test_loader)\n",
    "    \n",
    "    if epoch % (1) == 0:\n",
    "        summary.append('Train Epoch: {}\\tLoss: {:.6f}\\tTest Loss: {:.6f}\\tTest Acc: {:.6f} %'.format(epoch, train_loss[-1], test_loss[-1], acc))\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}\\tTest Loss: {:.6f}\\tTest Acc: {:.6f} %'.format(epoch, train_loss[-1], test_loss[-1], acc))\n",
    "\n",
    "    if invoke(early_stopping, test_loss[-1], model, implement=True):\n",
    "        #model.load_state_dict(torch.load(os.path.join(results_dir,'11_protein_encoder'), map_location=device))\n",
    "        summary.append(f'Early stopping after {epoch} epochs')\n",
    "        break\n",
    "\n",
    "    #torch.save(model.state_dict(), os.path.join(results_dir, f'11_protein_encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c12bcf4f-cd32-4d7a-98ce-92099ea66b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 75, 405])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8260509-9419-4f3d-8b6b-08f487b4215b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
