{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edb270a-d110-429f-86db-cbfd43aa8e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanfunk/miniforge3/envs/erxn/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4db1267-15b7-4b13-8050-9dc4efef1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../data/ReactionSeq2Seq_Dataset/'\n",
    "\n",
    "training_data_path = os.path.join(PATH_TO_DATA, 'US_patents_1976-Sep2016_1product_reactions_train.csv')\n",
    "test_data_path = os.path.join(PATH_TO_DATA, 'US_patents_1976-Sep2016_1product_reactions_test.csv')\n",
    "valid_data_path = os.path.join(PATH_TO_DATA, 'US_patents_1976-Sep2016_1product_reactions_valid.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d06c2af8-f69d-4e28-8cde-5ae285949606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/pr3fylj57wlgrmcxg1v4zp3c0000gn/T/ipykernel_66881/2278991789.py:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(training_data_path, header=2, sep='\\t')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>CanonicalizedReaction</th>\n",
       "      <th>OriginalReaction</th>\n",
       "      <th>PatentNumber</th>\n",
       "      <th>ParagraphNum</th>\n",
       "      <th>Year</th>\n",
       "      <th>TextMinedYield</th>\n",
       "      <th>CalculatedYield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C C S ( = O ) ( = O ) Cl . O C C Br &gt; A_CCN(CC...</td>\n",
       "      <td>C C S ( = O ) ( = O ) O C C Br</td>\n",
       "      <td>CCS(=O)(=O)Cl.OCCBr&gt;CCN(CC)CC.CCOCC&gt;CCS(=O)(=O...</td>\n",
       "      <td>[Br:1][CH2:2][CH2:3][OH:4].[CH2:5]([S:7](Cl)(=...</td>\n",
       "      <td>US03930836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C C ( C ) C S ( = O ) ( = O ) Cl . O C C Cl &gt; ...</td>\n",
       "      <td>C C ( C ) C S ( = O ) ( = O ) O C C Cl</td>\n",
       "      <td>CC(C)CS(=O)(=O)Cl.OCCCl&gt;CCN(CC)CC.CCOCC&gt;CC(C)C...</td>\n",
       "      <td>[CH2:1]([Cl:4])[CH2:2][OH:3].CCOCC.[CH2:10]([S...</td>\n",
       "      <td>US03930836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O = [N+] ( [O-] ) c 1 c c c c 2 c n c ( Cl ) c...</td>\n",
       "      <td>N c 1 c c c c 2 c n c ( Cl ) c c 1 2</td>\n",
       "      <td>O=[N+]([O-])c1cccc2cnc(Cl)cc12&gt;CC(=O)O.O.[Fe]....</td>\n",
       "      <td>[Cl:1][C:2]1[N:3]=[CH:4][C:5]2[C:10]([CH:11]=1...</td>\n",
       "      <td>US03930837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C c 1 c c 2 c ( [N+] ( = O ) [O-] ) c c c c 2 ...</td>\n",
       "      <td>C c 1 c c 2 c ( [N+] ( = O ) [O-] ) c c c c 2 ...</td>\n",
       "      <td>Cc1cc2c([N+](=O)[O-])cccc2c[n+]1[O-].O=P(Cl)(C...</td>\n",
       "      <td>[CH3:1][C:2]1[N+:3]([O-])=[CH:4][C:5]2[C:10]([...</td>\n",
       "      <td>US03930837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C C C C C [C@H] ( O ) C = C C 1 C = C C ( = O ...</td>\n",
       "      <td>C C C C C [C@H] ( O ) C = C C 1 C C C ( = O ) ...</td>\n",
       "      <td>CCCCC[C@H](O)C=CC1C=CC(=O)C1CC=CCCCC(=O)O&gt;CCO....</td>\n",
       "      <td>Cl.[OH:2][C@@H:3]([CH2:21][CH2:22][CH2:23][CH2...</td>\n",
       "      <td>US03930952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902576</th>\n",
       "      <td>C O c 1 c c c c ( C ( = N ) N c 2 c ( C ( C ) ...</td>\n",
       "      <td>C O c 1 c c c c ( - c 2 n c c n 2 - c 2 c ( C ...</td>\n",
       "      <td>COc1cccc(C(=N)Nc2c(C(C)C)cc(Br)cc2C(C)C)c1.O=C...</td>\n",
       "      <td>[Br:1][C:2]1[CH:7]=[C:6]([CH:8]([CH3:10])[CH3:...</td>\n",
       "      <td>US09450195B2</td>\n",
       "      <td>201</td>\n",
       "      <td>2016</td>\n",
       "      <td>79%</td>\n",
       "      <td>78.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902577</th>\n",
       "      <td>O = [N+] ( [O-] ) c 1 c c c c c 1 - c 1 c c c ...</td>\n",
       "      <td>Br c 1 c c c 2 c ( c 1 ) [nH] c 1 c c c c c 1 2</td>\n",
       "      <td>O=[N+]([O-])c1ccccc1-c1ccc(Br)cc1&gt;CCOP(OCC)OCC...</td>\n",
       "      <td>[Br:1][C:2]1[CH:7]=[CH:6][C:5]([C:8]2[CH:13]=[...</td>\n",
       "      <td>US09450195B2</td>\n",
       "      <td>204</td>\n",
       "      <td>2016</td>\n",
       "      <td>65%</td>\n",
       "      <td>65.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902578</th>\n",
       "      <td>Br c 1 c c c 2 c ( c 1 ) [nH] c 1 c c c c c 1 ...</td>\n",
       "      <td>Br c 1 c c c 2 c 3 c c c c c 3 n ( - c 3 c c c...</td>\n",
       "      <td>Brc1ccc2c(c1)[nH]c1ccccc12.Ic1ccccn1&gt;C1COCCO1....</td>\n",
       "      <td>[Br:1][C:2]1[CH:14]=[CH:13][C:12]2[C:11]3[C:6]...</td>\n",
       "      <td>US09450195B2</td>\n",
       "      <td>206</td>\n",
       "      <td>2016</td>\n",
       "      <td>43%</td>\n",
       "      <td>43.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902579</th>\n",
       "      <td>Br c 1 c c c 2 c 3 c c c c c 3 n ( - c 3 c c c...</td>\n",
       "      <td>I c 1 c c c 2 c 3 c c c c c 3 n ( - c 3 c c c ...</td>\n",
       "      <td>Brc1ccc2c3ccccc3n(-c3ccccn3)c2c1.[I-]&gt;C1COCCO1...</td>\n",
       "      <td>Br[C:2]1[CH:14]=[CH:13][C:12]2[C:11]3[C:6](=[C...</td>\n",
       "      <td>US09450195B2</td>\n",
       "      <td>207</td>\n",
       "      <td>2016</td>\n",
       "      <td>83%</td>\n",
       "      <td>83.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902580</th>\n",
       "      <td>C C ( C ) c 1 c c ( Br ) c c ( C ( C ) C ) c 1...</td>\n",
       "      <td>C C ( C ) c 1 c c ( B 2 O C ( C ) ( C ) C ( C ...</td>\n",
       "      <td>CC(C)c1cc(Br)cc(C(C)C)c1-n1ccnc1-c1cccc(Oc2ccc...</td>\n",
       "      <td>Br[C:2]1[CH:7]=[C:6]([CH:8]([CH3:10])[CH3:9])[...</td>\n",
       "      <td>US09450195B2</td>\n",
       "      <td>209</td>\n",
       "      <td>2016</td>\n",
       "      <td>90%</td>\n",
       "      <td>90.4%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>902581 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Source  \\\n",
       "0       C C S ( = O ) ( = O ) Cl . O C C Br > A_CCN(CC...   \n",
       "1       C C ( C ) C S ( = O ) ( = O ) Cl . O C C Cl > ...   \n",
       "2       O = [N+] ( [O-] ) c 1 c c c c 2 c n c ( Cl ) c...   \n",
       "3       C c 1 c c 2 c ( [N+] ( = O ) [O-] ) c c c c 2 ...   \n",
       "4       C C C C C [C@H] ( O ) C = C C 1 C = C C ( = O ...   \n",
       "...                                                   ...   \n",
       "902576  C O c 1 c c c c ( C ( = N ) N c 2 c ( C ( C ) ...   \n",
       "902577  O = [N+] ( [O-] ) c 1 c c c c c 1 - c 1 c c c ...   \n",
       "902578  Br c 1 c c c 2 c ( c 1 ) [nH] c 1 c c c c c 1 ...   \n",
       "902579  Br c 1 c c c 2 c 3 c c c c c 3 n ( - c 3 c c c...   \n",
       "902580  C C ( C ) c 1 c c ( Br ) c c ( C ( C ) C ) c 1...   \n",
       "\n",
       "                                                   Target  \\\n",
       "0                          C C S ( = O ) ( = O ) O C C Br   \n",
       "1                  C C ( C ) C S ( = O ) ( = O ) O C C Cl   \n",
       "2                    N c 1 c c c c 2 c n c ( Cl ) c c 1 2   \n",
       "3       C c 1 c c 2 c ( [N+] ( = O ) [O-] ) c c c c 2 ...   \n",
       "4       C C C C C [C@H] ( O ) C = C C 1 C C C ( = O ) ...   \n",
       "...                                                   ...   \n",
       "902576  C O c 1 c c c c ( - c 2 n c c n 2 - c 2 c ( C ...   \n",
       "902577    Br c 1 c c c 2 c ( c 1 ) [nH] c 1 c c c c c 1 2   \n",
       "902578  Br c 1 c c c 2 c 3 c c c c c 3 n ( - c 3 c c c...   \n",
       "902579  I c 1 c c c 2 c 3 c c c c c 3 n ( - c 3 c c c ...   \n",
       "902580  C C ( C ) c 1 c c ( B 2 O C ( C ) ( C ) C ( C ...   \n",
       "\n",
       "                                    CanonicalizedReaction  \\\n",
       "0       CCS(=O)(=O)Cl.OCCBr>CCN(CC)CC.CCOCC>CCS(=O)(=O...   \n",
       "1       CC(C)CS(=O)(=O)Cl.OCCCl>CCN(CC)CC.CCOCC>CC(C)C...   \n",
       "2       O=[N+]([O-])c1cccc2cnc(Cl)cc12>CC(=O)O.O.[Fe]....   \n",
       "3       Cc1cc2c([N+](=O)[O-])cccc2c[n+]1[O-].O=P(Cl)(C...   \n",
       "4       CCCCC[C@H](O)C=CC1C=CC(=O)C1CC=CCCCC(=O)O>CCO....   \n",
       "...                                                   ...   \n",
       "902576  COc1cccc(C(=N)Nc2c(C(C)C)cc(Br)cc2C(C)C)c1.O=C...   \n",
       "902577  O=[N+]([O-])c1ccccc1-c1ccc(Br)cc1>CCOP(OCC)OCC...   \n",
       "902578  Brc1ccc2c(c1)[nH]c1ccccc12.Ic1ccccn1>C1COCCO1....   \n",
       "902579  Brc1ccc2c3ccccc3n(-c3ccccn3)c2c1.[I-]>C1COCCO1...   \n",
       "902580  CC(C)c1cc(Br)cc(C(C)C)c1-n1ccnc1-c1cccc(Oc2ccc...   \n",
       "\n",
       "                                         OriginalReaction  PatentNumber  \\\n",
       "0       [Br:1][CH2:2][CH2:3][OH:4].[CH2:5]([S:7](Cl)(=...    US03930836   \n",
       "1       [CH2:1]([Cl:4])[CH2:2][OH:3].CCOCC.[CH2:10]([S...    US03930836   \n",
       "2       [Cl:1][C:2]1[N:3]=[CH:4][C:5]2[C:10]([CH:11]=1...    US03930837   \n",
       "3       [CH3:1][C:2]1[N+:3]([O-])=[CH:4][C:5]2[C:10]([...    US03930837   \n",
       "4       Cl.[OH:2][C@@H:3]([CH2:21][CH2:22][CH2:23][CH2...    US03930952   \n",
       "...                                                   ...           ...   \n",
       "902576  [Br:1][C:2]1[CH:7]=[C:6]([CH:8]([CH3:10])[CH3:...  US09450195B2   \n",
       "902577  [Br:1][C:2]1[CH:7]=[CH:6][C:5]([C:8]2[CH:13]=[...  US09450195B2   \n",
       "902578  [Br:1][C:2]1[CH:14]=[CH:13][C:12]2[C:11]3[C:6]...  US09450195B2   \n",
       "902579  Br[C:2]1[CH:14]=[CH:13][C:12]2[C:11]3[C:6](=[C...  US09450195B2   \n",
       "902580  Br[C:2]1[CH:7]=[C:6]([CH:8]([CH3:10])[CH3:9])[...  US09450195B2   \n",
       "\n",
       "       ParagraphNum  Year TextMinedYield CalculatedYield  \n",
       "0               NaN  1976            NaN             NaN  \n",
       "1               NaN  1976            NaN             NaN  \n",
       "2               NaN  1976            NaN             NaN  \n",
       "3               NaN  1976            NaN             NaN  \n",
       "4               NaN  1976            NaN             NaN  \n",
       "...             ...   ...            ...             ...  \n",
       "902576          201  2016            79%           78.6%  \n",
       "902577          204  2016            65%           65.5%  \n",
       "902578          206  2016            43%           43.2%  \n",
       "902579          207  2016            83%           83.1%  \n",
       "902580          209  2016            90%           90.4%  \n",
       "\n",
       "[902581 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(training_data_path, header=2, sep='\\t')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29869078-cd1c-4f45-b3e0-ee1139fe4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCS(=O)(=O)Cl.OCCBr>CCN(CC)CC.CCOCC>CCS(=O)(=O)OCCBr\n",
      "C C S ( = O ) ( = O ) Cl . O C C Br > C C N ( C C ) C C . C C O C C > C C S ( = O ) ( = O ) O C C Br\n",
      "\n",
      "CC(C)CS(=O)(=O)Cl.OCCCl>CCN(CC)CC.CCOCC>CC(C)CS(=O)(=O)OCCCl\n",
      "C C ( C ) C S ( = O ) ( = O ) Cl . O C C Cl > C C N ( C C ) C C . C C O C C > C C ( C ) C S ( = O ) ( = O ) O C C Cl\n",
      "\n",
      "O=[N+]([O-])c1cccc2cnc(Cl)cc12>CC(=O)O.O.[Fe].[Na+].[OH-]>Nc1cccc2cnc(Cl)cc12\n",
      "O = [N+] ( [O-] ) c 1 c c c c 2 c n c ( Cl ) c c 1 2 > C C ( = O ) O . O . [Fe] . [Na+] . [OH-] > N c 1 c c c c 2 c n c ( Cl ) c c 1 2\n",
      "\n",
      "Cc1cc2c([N+](=O)[O-])cccc2c[n+]1[O-].O=P(Cl)(Cl)Cl>>Cc1cc2c([N+](=O)[O-])cccc2c(Cl)n1\n",
      "C c 1 c c 2 c ( [N+] ( = O ) [O-] ) c c c c 2 c [n+] 1 [O-] . O = P ( Cl ) ( Cl ) Cl > > C c 1 c c 2 c ( [N+] ( = O ) [O-] ) c c c c 2 c ( Cl ) n 1\n",
      "\n",
      "CCCCC[C@H](O)C=CC1C=CC(=O)C1CC=CCCCC(=O)O>CCO.Cl>CCCCC[C@H](O)C=CC1CCC(=O)C1CC=CCCCC(=O)O\n",
      "C C C C C [C@H] ( O ) C = C C 1 C = C C ( = O ) C 1 C C = C C C C C ( = O ) O > C C O . Cl > C C C C C [C@H] ( O ) C = C C 1 C C C ( = O ) C 1 C C = C C C C C ( = O ) O\n",
      "\n",
      "CC(=O)OCC1=C(C(=O)O)N2C(=O)[C@@H](NC(=O)C(OC(C)=O)c3ccccc3)[C@H]2SC1>O>CC1=C(C(=O)O)N2C(=O)[C@@H](N)[C@H]2SC1\n",
      "C C ( = O ) O C C 1 = C ( C ( = O ) O ) N 2 C ( = O ) [C@@H] ( N C ( = O ) C ( O C ( C ) = O ) c 3 c c c c c 3 ) [C@H] 2 S C 1 > O > C C 1 = C ( C ( = O ) O ) N 2 C ( = O ) [C@@H] ( N ) [C@H] 2 S C 1\n",
      "\n",
      "COc1cccc(C2(CC(Cl)(Cl)Cl)CO2)c1.ClC(Cl)(Cl)CC1(c2ccc(Br)cc2)CO1>CCOc1cccc(C2(CC(Cl)(Cl)Cl)CO2)c1.ClC(Cl)(Cl)CC1(c2cccc(OCc3ccccc3)c2)CO1.Clc1ccc(C2(CC(Cl)(Cl)Cl)CO2)cc1.Clc1ccc(C2(CC(Cl)(Cl)Cl)CO2)cc1Cl.Clc1cccc(C2(CC(Cl)(Cl)Cl)CO2)c1.Fc1cccc(C2(CC(Cl)(Cl)Cl)CO2)c1>ClC(Cl)(Cl)CC1(c2cccc(Br)c2)CO1\n",
      "C O c 1 c c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c 1 . Cl C ( Cl ) ( Cl ) C C 1 ( c 2 c c c ( Br ) c c 2 ) C O 1 > C C O c 1 c c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c 1 . Cl C ( Cl ) ( Cl ) C C 1 ( c 2 c c c c ( O C c 3 c c c c c 3 ) c 2 ) C O 1 . Cl c 1 c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c c 1 . Cl c 1 c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c c 1 Cl . Cl c 1 c c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c 1 . F c 1 c c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c 1 > Cl C ( Cl ) ( Cl ) C C 1 ( c 2 c c c c ( Br ) c 2 ) C O 1\n",
      "\n",
      "COc1cc2ccccc2cc1C(=O)O.O=S(Cl)Cl>c1ccccc1>COc1cc2ccccc2cc1C(=O)Cl\n",
      "C O c 1 c c 2 c c c c c 2 c c 1 C ( = O ) O . O = S ( Cl ) Cl > c 1 c c c c c 1 > C O c 1 c c 2 c c c c c 2 c c 1 C ( = O ) Cl\n",
      "\n",
      "CCN(CC)CC.O.O=C(Cl)Oc1ccccc1>ClCCl.[Na+].[OH-]>Oc1ccccc1Cc1ccccc1O\n",
      "C C N ( C C ) C C . O . O = C ( Cl ) O c 1 c c c c c 1 > Cl C Cl . [Na+] . [OH-] > O c 1 c c c c c 1 C c 1 c c c c c 1 O\n",
      "\n",
      "CCOC(N)=O.Cc1ccc(N=C=O)cc1N=C=O>>O=C=NC1CCC(CC2CCC(N=C=O)CC2)CC1\n",
      "C C O C ( N ) = O . C c 1 c c c ( N = C = O ) c c 1 N = C = O > > O = C = N C 1 C C C ( C C 2 C C C ( N = C = O ) C C 2 ) C C 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    \"\"\"\n",
    "    Tokenize a SMILES molecule or reaction\n",
    "    \"\"\"\n",
    "    import re\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == ''.join(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "for i in range(10):\n",
    "    print(train_data['CanonicalizedReaction'][i])\n",
    "    print(smi_tokenizer(train_data['CanonicalizedReaction'][i]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fd566-5f98-43a8-b74a-0bbe8f643500",
   "metadata": {},
   "source": [
    "# Transformer implementation\n",
    "\n",
    "you can ignore that for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06006e0a-ae78-4c94-8c95-c60b4e854377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "62a32534-84a0-4bf0-8a2b-946633d3be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        return out\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,max_seq_len,embed_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_model_dim\n",
    "\n",
    "        pe = torch.zeros(max_seq_len,self.embed_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,self.embed_dim,2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "      \n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: dimension of embeding vector output\n",
    "            n_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim    #512 dim\n",
    "        self.n_heads = n_heads   #8\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  . each key,query, value will be of 64d\n",
    "       \n",
    "        #key,query and value matrixes    #64 x 64   \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512\n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n",
    "\n",
    "    def forward(self,key,query,value,mask=None):    #batch_size x sequence_length x embedding_dim    # 32 x 10 x 512\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key : key vector\n",
    "           query : query vector\n",
    "           value : value vector\n",
    "           mask: mask for decoder\n",
    "        \n",
    "        Returns:\n",
    "           output vector from multihead attention\n",
    "        \"\"\"\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        \n",
    "        # query dimension can change in decoder during inference. \n",
    "        # so we cant take general seq_length\n",
    "        seq_length_query = query.size(1)\n",
    "        \n",
    "        # 32x10x512\n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "       \n",
    "        k = self.key_matrix(key)       # (32x10x8x64)\n",
    "        q = self.query_matrix(query)   \n",
    "        v = self.value_matrix(value)\n",
    "\n",
    "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)\n",
    "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "       \n",
    "        # computes attention\n",
    "        # adjust key for matrix multiplication\n",
    "        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)\n",
    "        product = torch.matmul(q, k_adjusted)  #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)\n",
    "      \n",
    "        \n",
    "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
    "        if mask is not None:\n",
    "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        #divising by square root of key dimension\n",
    "        product = product / math.sqrt(self.single_head_dim) # / sqrt(64)\n",
    "\n",
    "        #applying softmax\n",
    "        scores = F.softmax(product, dim=-1)\n",
    " \n",
    "        #mutiply with value matrix\n",
    "        scores = torch.matmul(scores, v)  ##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) \n",
    "        \n",
    "        #concatenated output\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)\n",
    "        \n",
    "        output = self.out(concat) #(32,10,512) -> (32,10,512)\n",
    "       \n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim) \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,key,query,value):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           norm2_out: output of transformer block\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        attention_out = self.attention(key,query,value)  #32x10x512\n",
    "        attention_residual_out = attention_out + value  #32x10x512\n",
    "        norm1_out = self.dropout1(self.norm1(attention_residual_out)) #32x10x512\n",
    "\n",
    "        feed_fwd_out = self.feed_forward(norm1_out) #32x10x512 -> #32x10x2048 -> 32x10x512\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out #32x10x512\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out)) #32x10x512\n",
    "\n",
    "        return norm2_out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "        n_heads: number of heads in multihead attention\n",
    "        \n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        \n",
    "        # add cls token for classification\n",
    "        cls_token = self.cls_token.expand(out.shape[0], -1, -1)\n",
    "        src = torch.cat((cls_token, out), dim=1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out,out,out)\n",
    "\n",
    "        return out  #32x10x512\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
    "        \n",
    "    \n",
    "    def forward(self, key, query, x,mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attention \n",
    "        Returns:\n",
    "           out: output of transformer block\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        #we need to pass mask mask only to fst attention\n",
    "        attention = self.attention(x,x,x,mask=mask) #32x10x512\n",
    "        value = self.dropout(self.norm(attention + x))\n",
    "        \n",
    "        out = self.transformer_block(key, query, value)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           target_vocab_size: vocabulary size of taget\n",
    "           embed_dim: dimension of embedding\n",
    "           seq_len : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector from target\n",
    "            enc_out : output from encoder layer\n",
    "            trg_mask: mask for decoder self attention\n",
    "        Returns:\n",
    "            out: output vector\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        x = self.word_embedding(x)  #32x10x512\n",
    "        x = self.position_embedding(x) #32x10x512\n",
    "        x = self.dropout(x)\n",
    "     \n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask) \n",
    "\n",
    "        out = F.softmax(self.fc_out(x))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           embed_dim:  dimension of embedding \n",
    "           src_vocab_size: vocabulary size of source\n",
    "           target_vocab_size: vocabulary size of target\n",
    "           seq_length : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        \n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask    \n",
    "\n",
    "    def decode(self,src,trg):\n",
    "        \"\"\"\n",
    "        for inference\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out_labels : returns final prediction of sequence\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
    "        #outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)\n",
    "        out = trg\n",
    "        for i in range(seq_len): #10\n",
    "            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n",
    "            # taking the last token\n",
    "            out = out[:,-1,:]\n",
    "     \n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out,axis=0)\n",
    "          \n",
    "        \n",
    "        return out_labels\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out: final vector which returns probabilities of each target word\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        cls_token_final = enc_out[:, 0]\n",
    "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "17e1f8b6-12a1-4676-8579-86dae21f1e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12]) torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_layers = 6\n",
    "seq_length= 12\n",
    "\n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1], \n",
    "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
    "\n",
    "print(src.shape,target.shape)\n",
    "model = Transformer(embed_dim=512, \n",
    "                    src_vocab_size=src_vocab_size, \n",
    "                    target_vocab_size=target_vocab_size, \n",
    "                    seq_length=seq_length,\n",
    "                    num_layers=num_layers, \n",
    "                    expansion_factor=4, \n",
    "                    n_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68b20a56-70d5-457f-ace9-76c80e14ed65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 11])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(src, target)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "81b20108-2c81-46aa-959d-4ee4c8d51bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12]) torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "model = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, \n",
    "                    target_vocab_size=target_vocab_size, seq_length=seq_length, \n",
    "                    num_layers=num_layers, expansion_factor=4, n_heads=8)\n",
    "                  \n",
    "\n",
    "\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1]])\n",
    "trg = torch.tensor([[0]])\n",
    "print(src.shape,trg.shape)\n",
    "out = model.decode(src, trg)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b384d-f645-4524-a70a-f1964b84fd39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
