{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1edb270a-d110-429f-86db-cbfd43aa8e1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=6668): \n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "# conda create -n mol_transformer python=3.5\n",
    "# source activate mol_transformer\n",
    "# conda install rdkit -c rdkit\n",
    "# conda install future six tqdm pandas\n",
    "# conda install pytorch=0.4.1 torchvision -c pytorch\n",
    "# pip install torchtext==0.3.1\n",
    "# pip install -e .\n",
    "# conda install -c anaconda ipykernel\n",
    "# python -m ipykernel install --user --name=mol_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29869078-cd1c-4f45-b3e0-ee1139fe4f69",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCS(=O)(=O)Cl.OCCBr>CCN(CC)CC.CCOCC>CCS(=O)(=O)OCCBr\n",
      "C C S ( = O ) ( = O ) Cl . O C C Br > C C N ( C C ) C C . C C O C C > C C S ( = O ) ( = O ) O C C Br\n",
      "\n",
      "CC(C)CS(=O)(=O)Cl.OCCCl>CCN(CC)CC.CCOCC>CC(C)CS(=O)(=O)OCCCl\n",
      "C C ( C ) C S ( = O ) ( = O ) Cl . O C C Cl > C C N ( C C ) C C . C C O C C > C C ( C ) C S ( = O ) ( = O ) O C C Cl\n",
      "\n",
      "O=[N+]([O-])c1cccc2cnc(Cl)cc12>CC(=O)O.O.[Fe].[Na+].[OH-]>Nc1cccc2cnc(Cl)cc12\n",
      "O = [N+] ( [O-] ) c 1 c c c c 2 c n c ( Cl ) c c 1 2 > C C ( = O ) O . O . [Fe] . [Na+] . [OH-] > N c 1 c c c c 2 c n c ( Cl ) c c 1 2\n",
      "\n",
      "Cc1cc2c([N+](=O)[O-])cccc2c[n+]1[O-].O=P(Cl)(Cl)Cl>>Cc1cc2c([N+](=O)[O-])cccc2c(Cl)n1\n",
      "C c 1 c c 2 c ( [N+] ( = O ) [O-] ) c c c c 2 c [n+] 1 [O-] . O = P ( Cl ) ( Cl ) Cl > > C c 1 c c 2 c ( [N+] ( = O ) [O-] ) c c c c 2 c ( Cl ) n 1\n",
      "\n",
      "CCCCC[C@H](O)C=CC1C=CC(=O)C1CC=CCCCC(=O)O>CCO.Cl>CCCCC[C@H](O)C=CC1CCC(=O)C1CC=CCCCC(=O)O\n",
      "C C C C C [C@H] ( O ) C = C C 1 C = C C ( = O ) C 1 C C = C C C C C ( = O ) O > C C O . Cl > C C C C C [C@H] ( O ) C = C C 1 C C C ( = O ) C 1 C C = C C C C C ( = O ) O\n",
      "\n",
      "CC(=O)OCC1=C(C(=O)O)N2C(=O)[C@@H](NC(=O)C(OC(C)=O)c3ccccc3)[C@H]2SC1>O>CC1=C(C(=O)O)N2C(=O)[C@@H](N)[C@H]2SC1\n",
      "C C ( = O ) O C C 1 = C ( C ( = O ) O ) N 2 C ( = O ) [C@@H] ( N C ( = O ) C ( O C ( C ) = O ) c 3 c c c c c 3 ) [C@H] 2 S C 1 > O > C C 1 = C ( C ( = O ) O ) N 2 C ( = O ) [C@@H] ( N ) [C@H] 2 S C 1\n",
      "\n",
      "COc1cccc(C2(CC(Cl)(Cl)Cl)CO2)c1.ClC(Cl)(Cl)CC1(c2ccc(Br)cc2)CO1>CCOc1cccc(C2(CC(Cl)(Cl)Cl)CO2)c1.ClC(Cl)(Cl)CC1(c2cccc(OCc3ccccc3)c2)CO1.Clc1ccc(C2(CC(Cl)(Cl)Cl)CO2)cc1.Clc1ccc(C2(CC(Cl)(Cl)Cl)CO2)cc1Cl.Clc1cccc(C2(CC(Cl)(Cl)Cl)CO2)c1.Fc1cccc(C2(CC(Cl)(Cl)Cl)CO2)c1>ClC(Cl)(Cl)CC1(c2cccc(Br)c2)CO1\n",
      "C O c 1 c c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c 1 . Cl C ( Cl ) ( Cl ) C C 1 ( c 2 c c c ( Br ) c c 2 ) C O 1 > C C O c 1 c c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c 1 . Cl C ( Cl ) ( Cl ) C C 1 ( c 2 c c c c ( O C c 3 c c c c c 3 ) c 2 ) C O 1 . Cl c 1 c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c c 1 . Cl c 1 c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c c 1 Cl . Cl c 1 c c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c 1 . F c 1 c c c c ( C 2 ( C C ( Cl ) ( Cl ) Cl ) C O 2 ) c 1 > Cl C ( Cl ) ( Cl ) C C 1 ( c 2 c c c c ( Br ) c 2 ) C O 1\n",
      "\n",
      "COc1cc2ccccc2cc1C(=O)O.O=S(Cl)Cl>c1ccccc1>COc1cc2ccccc2cc1C(=O)Cl\n",
      "C O c 1 c c 2 c c c c c 2 c c 1 C ( = O ) O . O = S ( Cl ) Cl > c 1 c c c c c 1 > C O c 1 c c 2 c c c c c 2 c c 1 C ( = O ) Cl\n",
      "\n",
      "CCN(CC)CC.O.O=C(Cl)Oc1ccccc1>ClCCl.[Na+].[OH-]>Oc1ccccc1Cc1ccccc1O\n",
      "C C N ( C C ) C C . O . O = C ( Cl ) O c 1 c c c c c 1 > Cl C Cl . [Na+] . [OH-] > O c 1 c c c c c 1 C c 1 c c c c c 1 O\n",
      "\n",
      "CCOC(N)=O.Cc1ccc(N=C=O)cc1N=C=O>>O=C=NC1CCC(CC2CCC(N=C=O)CC2)CC1\n",
      "C C O C ( N ) = O . C c 1 c c c ( N = C = O ) c c 1 N = C = O > > O = C = N C 1 C C C ( C C 2 C C C ( N = C = O ) C C 2 ) C C 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization (already done in /mol_transf/MIT_mixed)\n",
    "# def smi_tokenizer(smi):\n",
    "#     \"\"\"\n",
    "#     Tokenize a SMILES molecule or reaction\n",
    "#     \"\"\"\n",
    "#     import re\n",
    "#     pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "#     regex = re.compile(pattern)\n",
    "#     tokens = [token for token in regex.findall(smi)]\n",
    "#     assert smi == ''.join(tokens)\n",
    "#     return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Augmentation (already done in /mol_transf/MIT_mixed_augm)\n",
    "# from rdkit import Chem\n",
    "# smi = ''\n",
    "# random_equivalent_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smi), doRandom=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"preprocess.py\", line 13, in <module>\n",
      "    import torch\n",
      "ImportError: No module named torch\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'# Input file generation (from augmented data)\\ndataset=MIT_mixed_augm # MIT_mixed_augm / STEREO_mixed_augm\\n\\npython preprocess.py -train_src ../data/eric_mol_transf/${dataset}/src-train.txt \\\\\\n                     -train_tgt ../data/eric_mol_transf/${dataset}/tgt-train.txt \\\\\\n                     -valid_src ../data/eric_mol_transf/${dataset}/src-val.txt \\\\\\n                     -valid_tgt ../data/eric_mol_transf/${dataset}/tgt-val.txt \\\\\\n                     -save_data ../data/eric_mol_transf/${dataset}/${dataset} \\\\\\n                     -src_seq_length 1000 -tgt_seq_length 1000 \\\\\\n                     -src_vocab_size 1000 -tgt_vocab_size 1000 -share_vocab\\n\\n# We use a shared vocabulary. The vocab_size and seq_length are chosen to include the whole datasets.\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCalledProcessError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbash\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m# Input file generation (from augmented data)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mdataset=MIT_mixed_augm # MIT_mixed_augm / STEREO_mixed_augm\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mpython preprocess.py -train_src ../data/eric_mol_transf/$\u001B[39m\u001B[38;5;132;01m{dataset}\u001B[39;00m\u001B[38;5;124m/src-train.txt \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                     -train_tgt ../data/eric_mol_transf/$\u001B[39m\u001B[38;5;132;01m{dataset}\u001B[39;00m\u001B[38;5;124m/tgt-train.txt \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                     -valid_src ../data/eric_mol_transf/$\u001B[39m\u001B[38;5;132;01m{dataset}\u001B[39;00m\u001B[38;5;124m/src-val.txt \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                     -valid_tgt ../data/eric_mol_transf/$\u001B[39m\u001B[38;5;132;01m{dataset}\u001B[39;00m\u001B[38;5;124m/tgt-val.txt \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                     -save_data ../data/eric_mol_transf/$\u001B[39m\u001B[38;5;132;01m{dataset}\u001B[39;00m\u001B[38;5;124m/$\u001B[39m\u001B[38;5;132;01m{dataset}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                     -src_seq_length 1000 -tgt_seq_length 1000 \u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                     -src_vocab_size 1000 -tgt_vocab_size 1000 -share_vocab\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m# We use a shared vocabulary. The vocab_size and seq_length are chosen to include the whole datasets.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2362\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2360\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2361\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2362\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/magics/script.py:153\u001B[0m, in \u001B[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001B[0;34m(line, cell)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    152\u001B[0m     line \u001B[38;5;241m=\u001B[39m script\n\u001B[0;32m--> 153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshebang\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcell\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/magics/script.py:305\u001B[0m, in \u001B[0;36mScriptMagics.shebang\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mraise_error \u001B[38;5;129;01mand\u001B[39;00m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001B[39;00m\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001B[39;00m\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001B[39;00m\n\u001B[1;32m    304\u001B[0m     rc \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m9\u001B[39m\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CalledProcessError(rc, cell)\n",
      "\u001B[0;31mCalledProcessError\u001B[0m: Command 'b'# Input file generation (from augmented data)\\ndataset=MIT_mixed_augm # MIT_mixed_augm / STEREO_mixed_augm\\n\\npython preprocess.py -train_src ../data/eric_mol_transf/${dataset}/src-train.txt \\\\\\n                     -train_tgt ../data/eric_mol_transf/${dataset}/tgt-train.txt \\\\\\n                     -valid_src ../data/eric_mol_transf/${dataset}/src-val.txt \\\\\\n                     -valid_tgt ../data/eric_mol_transf/${dataset}/tgt-val.txt \\\\\\n                     -save_data ../data/eric_mol_transf/${dataset}/${dataset} \\\\\\n                     -src_seq_length 1000 -tgt_seq_length 1000 \\\\\\n                     -src_vocab_size 1000 -tgt_vocab_size 1000 -share_vocab\\n\\n# We use a shared vocabulary. The vocab_size and seq_length are chosen to include the whole datasets.\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Input file generation (from augmented data)\n",
    "dataset=MIT_mixed_augm # MIT_mixed_augm / STEREO_mixed_augm\n",
    "\n",
    "python preprocess.py -train_src data/${dataset}/src-train.txt \\\n",
    "                     -train_tgt data/${dataset}/tgt-train.txt \\\n",
    "                     -valid_src data/${dataset}/src-val.txt \\\n",
    "                     -valid_tgt data/${dataset}/tgt-val.txt \\\n",
    "                     -save_data data/eric_mol_transf/${dataset}/${dataset} \\\n",
    "                     -src_seq_length 1000 -tgt_seq_length 1000 \\\n",
    "                     -src_vocab_size 1000 -tgt_vocab_size 1000 -share_vocab\n",
    "\n",
    "# We use a shared vocabulary. The vocab_size and seq_length are chosen to include the whole datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training phase (from preprocessed augmented data)\n",
    "dataset=MIT_mixed_augm # MIT_mixed_augm / STEREO_mixed_augm\n",
    "\n",
    "# Our MIT models were trained for 48 hours on a single GPU (STEREO for 72h), using the following hyperparameters:\n",
    "python  train.py -data data/${dataset}/${dataset} \\\n",
    "                   -save_model experiments/checkpoints/${dataset}/${dataset}_model \\\n",
    "                   -seed 42 -gpu_ranks 0 -save_checkpoint_steps 10000 -keep_checkpoint 20 \\\n",
    "                   -train_steps 500000 -param_init 0  -param_init_glorot -max_generator_batches 32 \\\n",
    "                   -batch_size 4096 -batch_type tokens -normalization tokens -max_grad_norm 0  -accum_count 4 \\\n",
    "                   -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam -warmup_steps 8000  \\\n",
    "                   -learning_rate 2 -label_smoothing 0.0 -report_every 1000 \\\n",
    "                   -layers 4 -rnn_size 256 -word_vec_size 256 -encoder_type transformer -decoder_type transformer \\\n",
    "                   -dropout 0.1 -position_encoding -share_embeddings \\\n",
    "                   -global_attention general -global_attention_function softmax -self_attn_type scaled-dot \\\n",
    "                   -heads 8 -transformer_ff 2048\n",
    "\n",
    "# To achieve the best results with single models, we average the last 20 checkpoints."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing phase (from averaged model and MIT mixed augmented data)\n",
    "model=${dataset}_model_average_20.pt\n",
    "\n",
    "python translate.py -model experiments/models/${model} \\\n",
    "                    -src data/${dataset}/eric_test/short_src-test.txt \\\n",
    "                    -output experiments/results/eric_results/predictions_${model}_on_${dataset}_test.txt \\\n",
    "                    -batch_size 64 -replace_unk -max_length 200 -fast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluation phase (from averaged model and MIT mixed augmented data)\n",
    "# Run the following script to get the top-1 accuracy.\n",
    "\n",
    "python score_predictions.py -targets data/${dataset}/eric_test/short_tgt-test.txt \\\n",
    "                    -predictions experiments/results/eric_results/predictions_${model}_on_${dataset}_test.txt -beam_size 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}